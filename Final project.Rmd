– Model selection and post-selection inference: In regression modeling, we sometimes want to include only a subset of our variables in the model and would like that decision to be data dependent. 
Two solutions to this problem are forward stepwise selection and backward stepwise selection, in which you fit a sequence of models, either adding or subtracting one variable at a time until some stopping criterion is reached. 
These procedures work for variable selection, but they invalidate the standard methods of inference in linear models. 
2 You will: 
– Write code implementing either forward stepwise selection or backward stepwise selection. 
 – Set up a simulation experiment, run your method on simulated data, obtain p-values or confidence intervals in the selected model.
----------------------------------


#data generating
-Set the model
```{r}
#simple setup: standard linear model Y=Xβ+ε. 

set.seed(610)

n <- 100
p <- 10 

X <- matrix(rnorm(n*p), n, p)
beta <- c(1, -1, rep(0, p-2))
e <- rnorm(n)
y <- X %*% beta + e  
```


#implement stepwise: backward stepwise selection 
-add or remove one variable at a time 
```{r}
#approach: remove one predictor at a time based on the smallest RSS

backward_stepwise <- function(dat) {
  cols <- names(dat)
  pred <- cols[cols != "y"]

  while (TRUE) {
    k <- length(pred)
    if (k == 0) break

    fm <- paste(pred, collapse = "+")
    if (fm == "") break

    form <- as.formula(paste("y ~", fm))
    fit <- lm(form, dat)

    if (k == 1) break

    rss_vec <- numeric(k)
    for (j in 1:k) {
      new_pred <- pred[-j]
      new_fm <- paste(new_pred, collapse = "+")
      new_form <- as.formula(paste("y ~", new_fm))
      new_fit <- lm(new_form, dat)
      rss_vec[j] <- sum(new_fit$residuals^2)
    }

    j_min <- which.min(rss_vec)
    pred <- pred[-j_min]
  }

  fit
}
```


#Simulation with the final model
```{r}
#sim
set.seed(610)
n <- 100
p <- 10
beta_true <- c(1, -1, rep(0, p - 2))

simulation <- 1000

selected_list <- vector("list", simulation)
sim_mat <- matrix(0, nrow = simulation, ncol = p)
colnames(sim_mat) <- paste0("X", 1:p)

results_list <- vector("list", simulation)

for (i in 1:simulation) {
  X <- matrix(rnorm(n * p), n, p)
  e <- rnorm(n)
  y <- X %*% beta_true + e  

  dat <- data.frame(y = as.numeric(y), X)
  colnames(dat) <- c("y", paste0("X", 1:p))

  fit <- backward_stepwise(dat)

  vars <- names(coef(fit))[-1]
  selected_list[[i]] <- vars
  if (length(vars) > 0) {
    sim_mat[i, vars] <- 1
  }

  sm <- summary(fit)$coefficients

  # X1
  if ("X1" %in% rownames(sm)) {
    b1 <- sm["X1", "Estimate"]
    s1 <- sm["X1", "Std. Error"]
    p1 <- sm["X1", "Pr(>|t|)"]
  } else {
    b1 <- NA
    s1 <- NA
    p1 <- NA
  }

  # X2
  if ("X2" %in% rownames(sm)) {
    b2 <- sm["X2", "Estimate"]
    s2 <- sm["X2", "Std. Error"]
    p2 <- sm["X2", "Pr(>|t|)"]
  } else {
    b2 <- NA
    s2 <- NA
    p2 <- NA
  }

  results_list[[i]] <- data.frame(
    sim = i,
    term = c("X1", "X2"),
    beta_hat = c(b1, b2),
    se = c(s1, s2),
    p = c(p1, p2)
  )
}

results <- do.call(rbind, results_list)

```

#Summarize and interpret results
