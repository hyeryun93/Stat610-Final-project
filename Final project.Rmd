Model selection and post-selection inference: In regression modeling, we sometimes want to
include only a subset of our variables in the model and would like that decision to be data
dependent. Two solutions to this problem are forward stepwise selection and backward stepwise
selection, in which you fit a sequence of models, either adding or subtracting one variable
at a time until some stopping criterion is reached. These procedures work for variable
selection, but they invalidate the standard methods of inference in linear models.
You will:
– Write code implementing either forward stepwise selection or backward stepwise selection.
– Set up a simulation experiment, run your method on simulated data, obtain p-values
or confidence intervals in the selected model, and report on whether the hypothesis
tests and confidence intervals were valid.
----------------------------------


#data generating
-Set the model
```{r}
#simple setup: standard linear model Y=Xβ+ε. 

set.seed(610)

n <- 100
p <- 10 

X <- matrix(rnorm(n*p), n, p)
beta <- c(1, -1, rep(0, p-2))
e <- rnorm(n)
y <- X %*% beta + e  
```


#implement stepwise: backward stepwise selection 
approach: AIC 
```{r}
 
backward_stepwise <- function(dat) {
  all_vars <- colnames(dat)[colnames(dat) != "y"]

  #model generating
  formula <- paste("y ~", paste(all_vars, collapse = "+"))
  model <- lm(formula, dat)
  current_AIC <- AIC(model)

  #removing variables
  while (length(all_vars) > 1) {
    AIC_val <- numeric(length(all_vars))   

    #AIC
    for (i in 1:length(all_vars)) {
      reduced_vars <- all_vars[-i]   
      reduced_formula <- paste("y ~", paste(reduced_vars, collapse = "+"))
      reduced_model <- lm(reduced_formula, dat)

      AIC_val[i] <- AIC(reduced_model) 
    }

    min_AIC <- min(AIC_val)   
    
    if (min_AIC < current_AIC) {
      worst_variable <- all_vars[which.min(AIC_val)]
      all_vars <- all_vars[all_vars != worst_variable]   
      current_AIC <- min_AIC   

    } else {

      break
    }
  }

  #final model
  final_formula <- paste("y ~", paste(all_vars, collapse = "+"))
  final_model <- lm(final_formula, dat)
  return(final_model)
}
final_model
```


 
#simulation1 
```{r}
#simulation setup
set.seed(610)

n <- 100
p <- 10
beta_true <- c(1, -1, rep(0, p-2))
alpha <- 0.05
simulation_num <- 1000
var_names <- paste0("X", 1:p)
null_vars <- var_names[3:p]

#initialize storage 

selected_matrix <- matrix(0, nrow = simulation_num, ncol = p)
colnames(selected_matrix) <- var_names

pval_X1 <- rep(NA, simulation_num)
pval_X2 <- rep(NA, simulation_num)
typeI_error <- rep(0, simulation_num)

#main loop
for(sim in 1:simulation_num) {
  X <- matrix(rnorm(n * p), n, p)
  e <- rnorm(n)
  y <- X %*% beta_true + e


#initialize data frame   
  dat <- data.frame(y = as.numeric(y))  
  for(i in 1:p) {
      dat[[paste0("X", i)]] <- X[, i]
  }
  
  # Model Selection
  final_model <- backward_stepwise(dat)
  selected_vars <- rownames(summary(final_model)$coefficients)[-1]
  
  if(length(selected_vars) > 0) {
    for(var in selected_vars) {
      selected_matrix[sim, which(var_names == var)] <- 1
    }
  }
coef_summary <- summary(final_model)$coefficients
```
